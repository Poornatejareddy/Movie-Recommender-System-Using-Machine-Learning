{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d9d44d0",
      "metadata": {
        "id": "2d9d44d0"
      },
      "source": [
        "# Real-time Movie Recommender System (Python-only, free)\n",
        "\n",
        "This notebook builds an end-to-end **Python-only** recommender using the free MovieLens 100k dataset.\n",
        "\n",
        "Features:\n",
        "- Downloads MovieLens 100k (no paid services)\n",
        "- Popular baseline, SVD (Surprise) collaborative model\n",
        "- Content-based TF-IDF on titles + genres\n",
        "- **Real-time** user profile updating (instant recommendations without retraining): user profile = weighted average of item embeddings (from SVD or TF-IDF)\n",
        "- Optional minimal FastAPI example to serve recommendations locally\n",
        "\n",
        "Run each cell in order. The notebook is intended to run locally (internet required for the dataset download)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6834d0da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6834d0da",
        "outputId": "7caca8ef-9990-4f90-e615-92fd0d0a2818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dependencies installed (or were already present).\n"
          ]
        }
      ],
      "source": [
        "# Install required packages. Run this cell once.\n",
        "import sys, subprocess\n",
        "\n",
        "packages = [\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"scipy\",\n",
        "    \"scikit-learn\",\n",
        "    \"scikit-surprise\",     # âœ… correct name instead of surprise==1.1.3\n",
        "    \"implicit\",\n",
        "    \"joblib\",\n",
        "    \"tqdm\",\n",
        "    \"fastapi\",\n",
        "    \"uvicorn\",\n",
        "    \"nbformat\",\n",
        "    \"requests\"\n",
        "]\n",
        "\n",
        "for p in packages:\n",
        "    name = p.split(\"==\")[0]\n",
        "    try:\n",
        "        __import__(name.replace(\"-\", \"_\"))  # e.g., scikit-surprise -> scikit_surprise\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
        "\n",
        "print(\"âœ… Dependencies installed (or were already present).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "69d978f0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69d978f0",
        "outputId": "73de25e9-068b-4055-90f7-32a3a017e79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found:\n",
            " - data/ml-100k/ml-100k/u.data\n",
            " - data/ml-100k/ml-100k/u.item\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3642765914.py:43: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  ratings = pd.read_csv(udata, sep=\"\\\\t\", names=ratings_cols, encoding='latin-1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ratings: (100000, 4) | Movies: (1682, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   user_id  movie_id  rating  timestamp\n",
              " 0      196       242       3  881250949\n",
              " 1      186       302       3  891717742\n",
              " 2       22       377       1  878887116\n",
              " 3      244        51       2  880606923\n",
              " 4      166       346       1  886397596,\n",
              "    movie_id              title       genres\n",
              " 0         1   Toy Story (1995)  01-Jan-1995\n",
              " 1         2   GoldenEye (1995)  01-Jan-1995\n",
              " 2         3  Four Rooms (1995)  01-Jan-1995\n",
              " 3         4  Get Shorty (1995)  01-Jan-1995\n",
              " 4         5     Copycat (1995)  01-Jan-1995)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Robust MovieLens 100k downloader and loader (works even if files are in ml-100k/ml-100k/)\n",
        "import os, zipfile, pandas as pd, requests\n",
        "\n",
        "DATA_DIR = \"data/ml-100k\"\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "zip_url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
        "zip_path = os.path.join(DATA_DIR, \"ml-100k.zip\")\n",
        "\n",
        "def find_ml_files(base_dir):\n",
        "    \"\"\"Find u.data and u.item files anywhere under base_dir.\"\"\"\n",
        "    udata_path = None\n",
        "    uitem_path = None\n",
        "    for root, _, files in os.walk(base_dir):\n",
        "        if \"u.data\" in files:\n",
        "            udata_path = os.path.join(root, \"u.data\")\n",
        "        if \"u.item\" in files:\n",
        "            uitem_path = os.path.join(root, \"u.item\")\n",
        "        if udata_path and uitem_path:\n",
        "            break\n",
        "    return udata_path, uitem_path\n",
        "\n",
        "udata, uitem = find_ml_files(DATA_DIR)\n",
        "\n",
        "if not (udata and uitem):\n",
        "    print(\"Downloading MovieLens 100k (~5MB)...\")\n",
        "    r = requests.get(zip_url, timeout=30)\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        z.extractall(DATA_DIR)\n",
        "    print(\"Extracted to\", DATA_DIR)\n",
        "    udata, uitem = find_ml_files(DATA_DIR)\n",
        "\n",
        "if not (udata and uitem):\n",
        "    raise FileNotFoundError(f\"Could not find u.data or u.item in {DATA_DIR}. Please check the folder structure.\")\n",
        "\n",
        "print(\"Found:\")\n",
        "print(\" -\", udata)\n",
        "print(\" -\", uitem)\n",
        "\n",
        "# Load ratings and movies\n",
        "ratings_cols = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
        "ratings = pd.read_csv(udata, sep=\"\\\\t\", names=ratings_cols, encoding='latin-1')\n",
        "\n",
        "movies_cols = [\"movie_id\", \"title\", \"genres\"]\n",
        "movies_raw = pd.read_csv(uitem, sep=\"|\", names=list(range(24)), encoding='latin-1', engine=\"python\")\n",
        "movies = movies_raw[[0, 1, 2]].copy()\n",
        "movies.columns = movies_cols\n",
        "movies['genres'] = movies['genres'].fillna('')\n",
        "\n",
        "print(\"âœ… Ratings:\", ratings.shape, \"| Movies:\", movies.shape)\n",
        "ratings.head(), movies.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "616e30c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "616e30c7",
        "outputId": "39d3ac52-57d5-4773-d5d2-b905b70daba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (99057, 6) Test size: (943, 6)\n"
          ]
        }
      ],
      "source": [
        "# Basic preprocessing: map ids to ints, build train/test (leave-last-out)\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Map ids to contiguous ints (optional but convenient)\n",
        "unique_users = ratings['user_id'].unique()\n",
        "unique_movies = ratings['movie_id'].unique()\n",
        "user_map = {u:i for i,u in enumerate(unique_users)}\n",
        "movie_map = {m:i for i,m in enumerate(unique_movies)}\n",
        "ratings['user_idx'] = ratings['user_id'].map(user_map)\n",
        "ratings['movie_idx'] = ratings['movie_id'].map(movie_map)\n",
        "\n",
        "# Leave-last-out per user for evaluation (ranking): keep latest rating as test\n",
        "ratings = ratings.sort_values(['user_idx', 'timestamp'])\n",
        "test_rows = ratings.groupby('user_idx').tail(1).index\n",
        "train = ratings.drop(index=test_rows).reset_index(drop=True)\n",
        "test = ratings.loc[test_rows].reset_index(drop=True)\n",
        "\n",
        "print(\"Train size:\", train.shape, \"Test size:\", test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d4d3e164",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4d3e164",
        "outputId": "48ba8f89-8121-48ab-d649-89557f5ad6ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Trained TruncatedSVD and saved item embeddings. Shape: (1682, 50)\n"
          ]
        }
      ],
      "source": [
        "# Train Popular baseline and Matrix Factorization using TruncatedSVD (scikit-learn)\n",
        "from collections import Counter\n",
        "import joblib, os\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "# Popular baseline (by count)\n",
        "pop_counts = train['movie_idx'].value_counts().sort_values(ascending=False)\n",
        "popular_items = pop_counts.index.tolist()\n",
        "\n",
        "# Build userâ€“item sparse matrix\n",
        "user_item = coo_matrix(\n",
        "    (train['rating'], (train['user_idx'], train['movie_idx'])),\n",
        "    shape=(len(user_map), len(movie_map))\n",
        ")\n",
        "\n",
        "# Truncated SVD (Matrix Factorization)\n",
        "svd = TruncatedSVD(n_components=50, random_state=42)\n",
        "item_latent_aligned = svd.fit_transform(user_item.T)        # shape: n_items Ã— n_factors\n",
        "item_latent_aligned = normalize(item_latent_aligned, axis=1) # normalize for cosine similarity\n",
        "\n",
        "# Save model artifacts\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "joblib.dump({\n",
        "    'popular': popular_items,\n",
        "    'user_map': user_map,\n",
        "    'movie_map': movie_map,\n",
        "    'item_latent_aligned': item_latent_aligned\n",
        "}, \"models/mappings.pkl\")\n",
        "\n",
        "print(\"âœ… Trained TruncatedSVD and saved item embeddings. Shape:\", item_latent_aligned.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b01edd48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b01edd48",
        "outputId": "773b4c47-e092-4e05-f578-1ede4ec1073c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF item vectors shape: (1682, 2330)\n"
          ]
        }
      ],
      "source": [
        "# Content-based: TF-IDF on title + genres\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "import numpy as np\n",
        "\n",
        "movies['text'] = movies['title'].fillna('') + ' ' + movies['genres'].fillna('')\n",
        "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "item_tfidf = tfidf.fit_transform(movies['text'])  # shape: n_items x features\n",
        "\n",
        "# Keep dense item vectors for quick similarity with user profile.\n",
        "item_tfidf_dense = item_tfidf.toarray()\n",
        "print(\"TF-IDF item vectors shape:\", item_tfidf_dense.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b603bbae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b603bbae",
        "outputId": "e45b7c97-aa7f-4427-e44b-24fe49bce056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item latent factors (TruncatedSVD) shape: (1682, 50)\n",
            "âœ… Aligned and saved item latent factors successfully.\n"
          ]
        }
      ],
      "source": [
        "# Extract item latent factors from TruncatedSVD (scikit-learn)\n",
        "# Works without Surprise, using the precomputed matrix from the previous step\n",
        "import numpy as np\n",
        "import joblib, os\n",
        "\n",
        "# item_latent_aligned already computed by TruncatedSVD in the previous step\n",
        "# Itâ€™s shaped (n_items, n_factors) â€” each row corresponds to a movie_idx\n",
        "\n",
        "print(\"Item latent factors (TruncatedSVD) shape:\", item_latent_aligned.shape)\n",
        "\n",
        "# Optional: save for later steps or FastAPI serving\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "joblib.dump(item_latent_aligned, \"models/item_latent_aligned.pkl\")\n",
        "\n",
        "# Ensure we have the same movie index alignment\n",
        "assert item_latent_aligned.shape[0] == len(movie_map), \"Item embedding count mismatch with movie map\"\n",
        "print(\"âœ… Aligned and saved item latent factors successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "39d66b29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39d66b29",
        "outputId": "de13a0ea-5d82-43b3-dd46-c7a52d0e89e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommended movie indices (internal): [ 57  56  26  52  53  31   6 273 154  32]\n",
            "Recommendations (movie_id, title):\n",
            "423 E.T. the Extra-Terrestrial (1982) score=0.5883\n",
            "143 Sound of Music, The (1965) score=0.5618\n",
            "95 Aladdin (1992) score=0.5484\n",
            "181 Return of the Jedi (1983) score=0.5444\n",
            "196 Dead Poets Society (1989) score=0.5419\n",
            "98 Silence of the Lambs, The (1991) score=0.5376\n",
            "265 Hunt for Red October, The (1990) score=0.5193\n",
            "197 Graduate, The (1967) score=0.5117\n",
            "443 Birds, The (1963) score=0.5113\n",
            "193 Right Stuff, The (1983) score=0.5069\n"
          ]
        }
      ],
      "source": [
        "# Real-time recommender: user profile vector (no retraining) + nearest items by cosine similarity\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Normalize item embeddings\n",
        "item_latent_norm = normalize(item_latent_aligned, axis=1)\n",
        "item_tfidf_norm = normalize(item_tfidf_dense, axis=1)\n",
        "\n",
        "def build_user_profile_from_interactions(interacted_item_indices, weights=None, embedding='svd'):\n",
        "    # interacted_item_indices: list of movie_idx values that the user interacted with (positive feedback)\n",
        "    # weights: optional list of same length with weights (e.g., recency or rating)\n",
        "    # embedding: 'svd' or 'tfidf' or 'hybrid' (concatenate)\n",
        "    if len(interacted_item_indices) == 0:\n",
        "        return None\n",
        "    if embedding == 'svd':\n",
        "        vecs = item_latent_norm[interacted_item_indices]\n",
        "    elif embedding == 'tfidf':\n",
        "        vecs = item_tfidf_norm[interacted_item_indices]\n",
        "    elif embedding == 'hybrid':\n",
        "        vecs = np.hstack([item_latent_norm[interacted_item_indices], item_tfidf_norm[interacted_item_indices]])\n",
        "        vecs = normalize(vecs, axis=1)\n",
        "    else:\n",
        "        raise ValueError('unknown embedding')\n",
        "    if weights is None:\n",
        "        profile = vecs.mean(axis=0)\n",
        "    else:\n",
        "        w = np.array(weights).reshape(-1,1)\n",
        "        profile = (vecs * w).sum(axis=0) / w.sum()\n",
        "    profile = profile.reshape(1, -1)\n",
        "    profile = normalize(profile, axis=1)\n",
        "    return profile  # shape (1, dim)\n",
        "\n",
        "def recommend_for_profile(profile_vec, top_k=10, embedding='svd', exclude_indices=None):\n",
        "    if embedding == 'svd':\n",
        "        sims = cosine_similarity(profile_vec, item_latent_norm)[0]\n",
        "    elif embedding == 'tfidf':\n",
        "        sims = cosine_similarity(profile_vec, item_tfidf_norm)[0]\n",
        "    elif embedding == 'hybrid':\n",
        "        sims = cosine_similarity(profile_vec, np.hstack([item_latent_norm, item_tfidf_norm]))[0]\n",
        "    else:\n",
        "        raise ValueError('unknown embedding')\n",
        "    # Exclude already seen items\n",
        "    if exclude_indices is not None and len(exclude_indices)>0:\n",
        "        sims[exclude_indices] = -1\n",
        "    top_idx = sims.argsort()[::-1][:top_k]\n",
        "    return top_idx, sims[top_idx]\n",
        "\n",
        "# Example: simulate a new user who liked movie_idx [10, 50, 200]\n",
        "sample_interactions = [10, 50, 200]\n",
        "profile = build_user_profile_from_interactions(sample_interactions, embedding='hybrid')\n",
        "rec_idx, scores = recommend_for_profile(profile, top_k=10, embedding='hybrid', exclude_indices=sample_interactions)\n",
        "print('Recommended movie indices (internal):', rec_idx)\n",
        "inv_movie_map = {v:k for k,v in movie_map.items()}\n",
        "print('Recommendations (movie_id, title):')\n",
        "for idx, sc in zip(rec_idx, scores):\n",
        "    mid = inv_movie_map[idx]\n",
        "    title = movies.loc[movies['movie_id']==mid, 'title'].values[0]\n",
        "    print(mid, title, f'score={sc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f013ab31",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f013ab31",
        "outputId": "a27db89f-65c0-49b4-ad86-643ca62b71f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved embeddings and maps to models/\n"
          ]
        }
      ],
      "source": [
        "# Save item embeddings to disk for serving (optional)\n",
        "import numpy as np, joblib, os\n",
        "os.makedirs('models', exist_ok=True)\n",
        "np.save('models/item_latent_aligned.npy', item_latent_aligned)\n",
        "np.save('models/item_tfidf.npy', item_tfidf_dense)\n",
        "joblib.dump({'movie_id_to_idx': movie_map, 'idx_to_movie_id': {v:k for k,v in movie_map.items()}}, 'models/movie_maps.pkl')\n",
        "print('Saved embeddings and maps to models/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8ac95e61",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ac95e61",
        "outputId": "ee9cd295-44c7-41aa-80cd-fb535131695b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Precision@10 (hybrid profile): 0.010286320254506895\n"
          ]
        }
      ],
      "source": [
        "# Simple evaluation: Precision@K on leave-last-out test using profile-based ranking\n",
        "import numpy as np\n",
        "def precision_at_k_user(recommended_idxs, relevant_idxs, k=10):\n",
        "    return len(set(recommended_idxs[:k]) & set(relevant_idxs)) / k\n",
        "\n",
        "test_gt = test.groupby('user_idx')['movie_idx'].apply(list).to_dict()\n",
        "\n",
        "precisions = []\n",
        "for user_idx, gt_list in test_gt.items():\n",
        "    user_train_items = train[train['user_idx']==user_idx]['movie_idx'].tolist()\n",
        "    if len(user_train_items)==0:\n",
        "        continue\n",
        "    profile = build_user_profile_from_interactions(user_train_items, embedding='hybrid')\n",
        "    rec_idx, _ = recommend_for_profile(profile, top_k=10, embedding='hybrid', exclude_indices=user_train_items)\n",
        "    prec = precision_at_k_user(rec_idx, gt_list, k=10)\n",
        "    precisions.append(prec)\n",
        "print('Mean Precision@10 (hybrid profile):', np.mean(precisions))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ðŸŽ¬ Colab-Compatible Streamlit Movie Recommender (with ngrok auth)\n",
        "# ============================================================\n",
        "\n",
        "# STEP 1 â€” Install dependencies\n",
        "!pip install streamlit pyngrok -q\n",
        "\n",
        "# STEP 2 â€” Add your ngrok auth token\n",
        "!ngrok authtoken 32rWZvFnb6BC3GpnyvEdk0JJq7b_34DfL9ijFqPW9gnp8GGFV\n",
        "\n",
        "# STEP 3 â€” Paste your Streamlit code into a .py file\n",
        "app_code = r\"\"\"\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# ------------------------------\n",
        "# LOAD MODELS AND DATA\n",
        "# ------------------------------\n",
        "@st.cache_resource\n",
        "def load_models():\n",
        "    movie_maps = joblib.load(\"models/movie_maps.pkl\")\n",
        "    item_latent = np.load(\"models/item_latent_aligned.npy\")\n",
        "    item_tfidf = np.load(\"models/item_tfidf.npy\")\n",
        "    movies = pd.read_csv(\"data/ml-100k/ml-100k/u.item\", sep=\"|\", names=list(range(24)), encoding=\"latin-1\")[[0,1,2]]\n",
        "    movies.columns = [\"movie_id\", \"title\", \"genres\"]\n",
        "    return movie_maps, item_latent, item_tfidf, movies\n",
        "\n",
        "movie_maps, item_latent, item_tfidf, movies = load_models()\n",
        "\n",
        "movie_id_to_idx = movie_maps[\"movie_id_to_idx\"]\n",
        "idx_to_movie_id = movie_maps[\"idx_to_movie_id\"]\n",
        "\n",
        "# Normalize embeddings\n",
        "item_latent = normalize(item_latent, axis=1)\n",
        "item_tfidf = normalize(item_tfidf, axis=1)\n",
        "\n",
        "# ------------------------------\n",
        "# FUNCTIONS\n",
        "# ------------------------------\n",
        "def build_user_profile(selected_titles, embedding=\"hybrid\"):\n",
        "    indices = [movie_id_to_idx[int(movies[movies[\"title\"]==title][\"movie_id\"].values[0])]\n",
        "               for title in selected_titles if title in movies[\"title\"].values]\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "    if embedding == \"svd\":\n",
        "        vecs = item_latent[indices]\n",
        "    elif embedding == \"tfidf\":\n",
        "        vecs = item_tfidf[indices]\n",
        "    else:\n",
        "        vecs = np.hstack([item_latent[indices], item_tfidf[indices]])\n",
        "        vecs = normalize(vecs, axis=1)\n",
        "    profile = vecs.mean(axis=0).reshape(1, -1)\n",
        "    profile = normalize(profile, axis=1)\n",
        "    return profile\n",
        "\n",
        "def recommend(profile, embedding=\"hybrid\", top_k=10, exclude=None):\n",
        "    if embedding == \"svd\":\n",
        "        sims = cosine_similarity(profile, item_latent)[0]\n",
        "    elif embedding == \"tfidf\":\n",
        "        sims = cosine_similarity(profile, item_tfidf)[0]\n",
        "    else:\n",
        "        sims = cosine_similarity(profile, np.hstack([item_latent, item_tfidf]))[0]\n",
        "    if exclude:\n",
        "        for idx in exclude:\n",
        "            sims[idx] = -1\n",
        "    top_idx = sims.argsort()[::-1][:top_k]\n",
        "    recs = []\n",
        "    for i in top_idx:\n",
        "        mid = idx_to_movie_id[i]\n",
        "        title = movies.loc[movies[\"movie_id\"]==mid, \"title\"].values[0]\n",
        "        recs.append((title, sims[i]))\n",
        "    return recs\n",
        "\n",
        "# ------------------------------\n",
        "# STREAMLIT UI\n",
        "# ------------------------------\n",
        "st.set_page_config(page_title=\"Movie Recommender\", page_icon=\"ðŸŽ¬\")\n",
        "st.title(\"ðŸŽ¬ Real-Time Movie Recommender (Free, Local ML)\")\n",
        "st.markdown(\"Select movies you like â€” the system will recommend similar ones instantly!\")\n",
        "\n",
        "user_movies = st.multiselect(\n",
        "    \"ðŸŽ¥ Pick a few movies you liked:\",\n",
        "    movies[\"title\"].tolist(),\n",
        "    default=[\"Toy Story (1995)\", \"Pulp Fiction (1994)\"]\n",
        ")\n",
        "\n",
        "embedding_choice = st.radio(\"Model type:\", [\"hybrid\", \"svd\", \"tfidf\"], horizontal=True)\n",
        "\n",
        "if st.button(\"ðŸš€ Get Recommendations\"):\n",
        "    profile = build_user_profile(user_movies, embedding=embedding_choice)\n",
        "    if profile is None:\n",
        "        st.warning(\"Please select at least one valid movie.\")\n",
        "    else:\n",
        "        exclude = [movie_id_to_idx[int(movies[movies['title']==t]['movie_id'].values[0])] for t in user_movies]\n",
        "        recs = recommend(profile, embedding=embedding_choice, exclude=exclude)\n",
        "        st.subheader(\"ðŸŽ¯ Top Recommendations\")\n",
        "        for title, score in recs:\n",
        "            st.write(f\"**{title}** â€” similarity: `{score:.4f}`\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "\n",
        "# STEP 4 â€” Start Streamlit via ngrok tunnel\n",
        "from pyngrok import ngrok\n",
        "import threading, time\n",
        "\n",
        "def run_app():\n",
        "    !streamlit run app.py --server.port 8501 > /dev/null 2>&1\n",
        "\n",
        "thread = threading.Thread(target=run_app)\n",
        "thread.start()\n",
        "\n",
        "# Wait and open tunnel\n",
        "time.sleep(5)\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"âœ… Streamlit app is live at:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aWXZB-SOBy9",
        "outputId": "9c3cfe72-390a-45eb-8fd2-b36ab085d638"
      },
      "id": "5aWXZB-SOBy9",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "âœ… Streamlit app is live at: NgrokTunnel: \"https://ebbe6f8c5da3.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}